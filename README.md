# Complete solutions for all the assignments in the Stanford CS231n course.

This overview contains a dedicated section for each assignment, listing the main topics covered in it together with links to the notebooks where that topic is used. Additionally, each sub-section contains a "Topics Index" which aims to give a more in depth intuitive explanation of the most important topics covered in the assignment. This part also lists the core implementation section in the assignment relevant for that topic as well as relevant readings on the topic (either extracted from the notes or not) that can be useful in gaining more context or a better understanding of the topic.

## Assignment 1 - [Solutions Directory](./Assignments/assignment1/)

* Q1 - [k-Nearest Neighboor Classifier](./Assignments/assignment1/knn.ipynb)
* Q2 - [Training a Support Vector Machine](./Assignments/assignment1/svm.ipynb)
* Q3 - [Train a Softmax Classifier](./Assignments/assignment1/softmax.ipynb)
* Q4 - [Train a Two Layer Fully Connected Neural Network](./Assignments/assignment1/two_layer_net.ipynb)
* Q5 - [Higher Level Representations: Image Features](./Assignments/assignment1/features.ipynb)

## Assignment 2 - [Solutions Directory](./Assignments/assignment2/)

* Q1 - [Multi Layer Fully Connected Neural Networks](./Assignments/assignment2/FullyConnectedNets.ipynb)
* Q2 - [Batch Normalization](./Assignments/assignment2/BatchNormalization.ipynb)
* Q3 - [Dropout](./Assignments/assignment2/Dropout.ipynb)
* Q4 - [Convolutional Neural Networks](./Assignments/assignment2/ConvolutionalNetworks.ipynb)
* Q5 - [Pytorch on CIFAR10](./Assignments/assignment2/PyTorch.ipynb)

## Assignment 3 - [Solutions Directory](./Assignments/assignment3/)

* Q1 - [Network Visualization](./Assignments/assignment3/Network_Visualization.ipynb)
* Q2 - [Image Captioning with Vanilla RNNs (Recurrent Neural Networks)](./Assignments/assignment3/RNN_Captioning.ipynb)
* Q3 - [Image Captioning with LSTMs (Long Short Term Memory Networls)](./Assignments/assignment3/LSTM_Captioning.ipynb)
* Q4 - [Image Captioning with Transformers](./Assignments/assignment3/Transformer_Captioning.ipynb)
* Q5 - [GANs (Generative Adversial Networks)](./Assignments/assignment3/Generative_Adversarial_Networks.ipynb)
* Q6 - [Self Supervised (Contrastive) Learning](./Assignments/assignment3/Self_Supervised_Learning.ipynb)

## Topics Index
<style>
    table th:nth-child(2) { width: 25%; }
</style>

| Topic              | Relevant Reading                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Brief Intuitive Explanation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Implemented                                                                                 |
| :----------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------ |
| kNN  Classifier    | + [cs231n notes](https://cs231n.github.io/classification/) <br>+ [A Few Useful Things to Know About Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) <br>+ [Recognizing and Learning Object Categories](https://people.csail.mit.edu/torralba/shortCourseRLOC/index.html) <br>+[kNN classification using Neighbourhood Components Analysis](https://kevinzakka.github.io/2020/02/10/nca/) <br>+[PCA (Principal Component Analysis) and SVD(Singular Value Decomposition)](https://web.archive.org/web/20150503165118/http://www.bigdataexaminer.com:80/understanding-dimensionality-reduction-principal-component-analysis-and-singular-value-decomposition/) <br>+[Random Projections](https://scikit-learn.org/stable/modules/random_projection.html) <br>+[Approximate Nearest Neighboor Classifier - FLANN](https://github.com/mariusmuja/flann) | **Training** - This step can be replaced with storing all the available training data. k is hyperparameter that needs to be tuned on the validation data. <br> <br> **Inference** - The sample to classify is compared to all the available training samples by computing L1 norm ($d_1(I_1, I_2) = \sum_{p} \left\lVert I^p_1 - I^p_2 \right\rVert$) or L2 norm ($d_2 (I_1, I_2) = \sqrt{\sum_{p} \left( I^p_1 - I^p_2 \right)^2}$). The classifier assigns to the inference sample the most common class of the k training samples with the lowest distance to the inference sample.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | [k_nearest_neighbor.py](./Assignments/assignment1/cs231n/classifiers/k_nearest_neighbor.py) |
| SVM Classifier     | + [cs231n notes - Multiclass SVM Formulation](https://cs231n.github.io/linear-classify/) <br>+ [Inspiration paper for SVM formulation in the assignment](https://www.researchgate.net/publication/221166057_Support_Vector_Machines_for_Multi-Class_Pattern_Recognition) <br>+ [OVA (One-VS-All) SVM Formulation](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) <br>+ [AVA (All-VS-All) SVM Formulation](https://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.pdf) <br>+ [Structured SVM Formulation](https://www.cs.cornell.edu/people/tj/publications/joachims_etal_09b.pdf) <br>+ [Deep Learning using Support Vector Machines](https://arxiv.org/abs/1306.0239)                                                                                                                                                               | **Training** - The training stage aims to minimize the error of the classifier by tweaking the model weights so that they simmultaneusly can achieve the lowest possible loss when classifying samples in the training dataset. The error(loss) is non-zero whenever the predicted score for a class is different than the score of the correct class for a sample. Additionally, a regularization term is commonly used to explicitly limit the complexity of the model and prevent over-fitting. Intuitively, the training stage aims to find support vectors to define decision boundaries inbetween classes such that the distance between the support vector and the closest points of the classes it separates has a value of at least $\Delta$. It is common for $\Delta$ to have a value of 1. <br><br> **Activation Function** - For the single sample case (at sampling/inference) it is defined as a vector matrix multiplication $f(x_i; W) =  W x_i$ where the vector $x_i$ contains the sample data and the matrix $W$ contains the model weights for all classes that the classifier needs to distinguish amongst. For the multi sample case (at training) it is defined as a regular matrix multiplication $f(X; W) =  W X$ where one matrix ($X$) contains data for multiple samples and the second matrix ($W$) contains the model weights similar to the previous case. <br><br> **Loss Function** - For a single sample the loss function is known as "Hinge Loss" and is computed as $L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + \Delta)$. The overall loss function is computed as $L =  \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} + \underbrace{ \lambda R(W) }_\text{regularization loss} \\\\$ where $\lambda$ is the regularization strength hyperparameter and $R(W) = \sum_k\sum_l W_{k,l}^2$. In the regularization loss formula $W$ denotes the weight matrix of the SVM classifier and $N$ denotes the number of all training samples in the training dataset. <br><br> **Inference** - With the trained classifier, a class is assigned to a test sample based on the scores computed from the equation $f(x_i; W) =  W x_i$ on that sample for each of the classes. The assigned class is the class with the highest computed score. | [linear_svm.py](./Assignments/assignment1/cs231n/classifiers/linear_svm.py)                 |
| Softmax Classifier | + [cs231n notes - Softmax Classifier](https://cs231n.github.io/linear-classify/)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | **Training** - Similarly to the SVM, the training stage aims to minimize the loss function. The main difference between the Softmax and the SVM classifiers arise in the interpretation of the scores obtained from the activation function and the definition of the loss function. <br><br> **Activation Function** - The activation function is defined identically to the SVM for both training and inference stages. However, the interpretation of the class scores for a sample is different. In this case the scores are interpreted as **unnormalized log probabilities**. <br><br> **Loss Function** - The loss function for this classifier is known as the "Cross-Entropy Loss" and is defined as $L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}$ where $f_j$ is used to denote the $jth$ element in the vector of class scores $f$ and the function $f_j(z) = \frac{e^{z_j}}{\sum_k e^{z_k}}$ is known as the "Softmax Function". This function takes as input an array of real-values in an arbitrary range ($z$) and mapps it to an array of values between 0 and 1 that sum to 1 ($f(z)$). Intuitively, when minimizing the cross-entropy loss we aim to minimize the cross-entropy between the 2 distributions of data involved in its computation: the estimated class probabilities *predicted* by the model and the *real* class probabilities provided as labels. In other words, we can say that the cross-entropy objective wants the *predicted* distribution to have all of its mass on the correct answer. <br><br> **Inference** - With the trained classifier, a class is assigned to a test sample based on the scores computed from the equation $f(x_i; W) =  W x_i$ on that sample for each of the classes. The assigned class is the class with the highest computed score.                                                                                                                                                                                                                                                                                                                                                                   | [softmax.py](./Assignments/assignment1/cs231n/classifiers/softmax.py)                       |
| Backpropagation: Gradient Based Learning    | + [Optimization: Stochastic Gradient Descent](https://cs231n.github.io/optimization-1/) <br>+ [Backpropagation Intuitions](https://cs231n.github.io/optimization-2/) <br>+ [Automatic differentiation in machine learning: a survey](https://arxiv.org/abs/1502.05767) <br>+ [Vector, Matrix, and Tensor Derivatives](https://cs231n.stanford.edu/vecDerivs.pdf) <br>+ [The Matrix Calculus You Need For Deep Learning](https://arxiv.org/abs/1802.01528)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | **Intuition** - Gradient based learning implies that an objective function is defined. This objective is usually intended to measure the error of a model prediction. The model prediction is a function of model parameters and training data so naturally the objective function is going to be a function of model parameters and training data. Given that we want our model to have an error as low as possible we aim to minimize its error. In other words, we want to minimize the defined objective which measures how wrong the model prediction is. Since the data is fixed and we don't want it to change, we can achieve lower error by tweaking the model parameters. To know exactly how the model parameters should be tweaked to minimize the output of the objective we use its gradient. The gradient can be interpreted as the direction of the steepest ascent of the objective function but since we are interested in minmizing the objective we use its opposite value to update the model parameters. This enables us to find the decreasing direction of the objective function and update the model parameters along it to minimize the error. <br><br> **Gradient Computation** - Gradient of an expression can be computed either numerically (i.e. using the centered difference  formula) or analytically (using calculus rules). It is good practice to perform a gradient check after implementing the gradient function of an expression. This means evaluating both the numerical and analytical gradient expressions and comparing the results. We expect the results to be 0 (or at least in the order of $10^{-5}$). When dealing with intricate expressions it is common to decompose the expression into a "Circuit Diagram" and use the "Chain Rule" method to compute its gradient. <br><br> **Gradient Descent** - This reffers to the idea of computing the gradient of an expression and updating its parameters with the opposite of gradient value (when the goal is to minimize the objective value). It is common that the scaled gradient value is used for update and the scaling parameter is known as the "learning rate" which is a hyperparameter that needs to be tuned. |                                                                                             |
